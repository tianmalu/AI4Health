{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abede43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading labels: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19101/19101 [00:00<00:00, 2926162.41it/s]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "label_dict = {}\n",
    "with open(\"../ComParE2017_Cold_4students/lab/ComParE2017_Cold.tsv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f, delimiter=\"\\t\")\n",
    "    rows = list(reader)\n",
    "    for row in tqdm(rows, desc=\"Loading labels\"):\n",
    "        label_dict[row[\"file_name\"]] = row[\"Cold (upper respiratory tract infection)\"]\n",
    "\n",
    "def search_in_ground_truth(file_id: str, label_dict: dict) -> str:\n",
    "    wav_name = file_id + \".wav\"\n",
    "    return label_dict.get(wav_name, None)\n",
    "\n",
    "def search_in_labels(filename, label_dict):\n",
    "    base_name = os.path.splitext(filename)[0]\n",
    "    \n",
    "    if \"_logmel\" in base_name:\n",
    "        base_name = base_name.replace(\"_logmel\", \"\")\n",
    "    if \"_flipped\" in base_name:\n",
    "        base_name = base_name.replace(\"_flipped\", \"\")\n",
    "    \n",
    "    parts = base_name.split(\"_\")\n",
    "    if len(parts) >= 2:\n",
    "        audio_filename = f\"{parts[0]}_{parts[1]}.wav\"\n",
    "    else:\n",
    "        audio_filename = f\"{base_name}.wav\"\n",
    "    \n",
    "    return label_dict.get(audio_filename, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ce1bf92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Physical features loaded:\n",
      "  Total rows in CSV: 28652\n",
      "  Filtered rows: 19101\n",
      "  Features: ['filename', 'split', 'duration', 'mfcc_0_mean', 'mfcc_0_std', 'mfcc_1_mean', 'mfcc_1_std', 'mfcc_2_mean', 'mfcc_2_std', 'mfcc_3_mean', 'mfcc_3_std', 'mfcc_4_mean', 'mfcc_4_std', 'mfcc_5_mean', 'mfcc_5_std', 'mfcc_6_mean', 'mfcc_6_std', 'mfcc_7_mean', 'mfcc_7_std', 'mfcc_8_mean', 'mfcc_8_std', 'mfcc_9_mean', 'mfcc_9_std', 'mfcc_10_mean', 'mfcc_10_std', 'mfcc_11_mean', 'mfcc_11_std', 'mfcc_12_mean', 'mfcc_12_std', 'chroma_0_mean', 'chroma_0_std', 'chroma_1_mean', 'chroma_1_std', 'chroma_2_mean', 'chroma_2_std', 'chroma_3_mean', 'chroma_3_std', 'chroma_4_mean', 'chroma_4_std', 'chroma_5_mean', 'chroma_5_std', 'chroma_6_mean', 'chroma_6_std', 'chroma_7_mean', 'chroma_7_std', 'chroma_8_mean', 'chroma_8_std', 'chroma_9_mean', 'chroma_9_std', 'chroma_10_mean', 'chroma_10_std', 'chroma_11_mean', 'chroma_11_std', 'spectral_contrast_0_mean', 'spectral_contrast_0_std', 'spectral_contrast_1_mean', 'spectral_contrast_1_std', 'spectral_contrast_2_mean', 'spectral_contrast_2_std', 'spectral_contrast_3_mean', 'spectral_contrast_3_std', 'spectral_contrast_4_mean', 'spectral_contrast_4_std', 'spectral_contrast_5_mean', 'spectral_contrast_5_std', 'spectral_contrast_6_mean', 'spectral_contrast_6_std', 'spectral_flatness_mean', 'spectral_flatness_std', 'onset_strength_mean', 'onset_strength_std', 'zcr_mean', 'zcr_std', 'spectral_rolloff_mean', 'spectral_rolloff_std', 'spectral_centroid_mean', 'spectral_centroid_std']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_physical_features_as_df():\n",
    "    df = pd.read_csv(\"audio_features.csv\", delimiter=\",\", encoding=\"utf-8\")\n",
    "    \n",
    "    df_filtered = df[df['filename'].isin(label_dict.keys())]\n",
    "    \n",
    "    print(f\"üìä Physical features loaded:\")\n",
    "    print(f\"  Total rows in CSV: {len(df)}\")\n",
    "    print(f\"  Filtered rows: {len(df_filtered)}\")\n",
    "    print(f\"  Features: {list(df_filtered.columns)}\")\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "physical_features_df = load_physical_features_as_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d72cc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class SpectrogramDataset(Dataset):\n",
    "    def __init__(self, image_paths, label_dict, physical_features_df, transform=None, is_training=False):\n",
    "        self.image_paths = image_paths\n",
    "        self.label_dict = label_dict\n",
    "        self.is_training = is_training \n",
    "        self.physical_features_df = physical_features_df\n",
    "        \n",
    "        self.prepare_physical_features()\n",
    "        \n",
    "        self.base_transform = transforms.Compose([\n",
    "            transforms.Resize((210, 70)),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.3, 0)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        self.c_train_transform = transforms.Compose([\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.3, 0)),\n",
    "            transforms.Resize((210, 70)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        \n",
    "    def prepare_physical_features(self):\n",
    "        numeric_columns = self.physical_features_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "        columns_to_remove = ['split'] if 'split' in numeric_columns else []\n",
    "        for col in columns_to_remove:\n",
    "            numeric_columns.remove(col)\n",
    "        \n",
    "        self.feature_columns = numeric_columns\n",
    "        \n",
    "        self.features_dict = {}\n",
    "        for _, row in self.physical_features_df.iterrows():\n",
    "            filename = row['filename']\n",
    "            features = row[self.feature_columns].values.astype(np.float32)\n",
    "            features = np.nan_to_num(features, nan=0.0)\n",
    "            self.features_dict[filename] = torch.tensor(features, dtype=torch.float32)\n",
    "        \n",
    "        print(f\"üìä Physical features prepared:\")\n",
    "        print(f\"  Feature dimensions: {len(self.feature_columns)}\")\n",
    "        \n",
    "    def get_physical_features(self, image_filename):\n",
    "        base_name = os.path.splitext(image_filename)[0]\n",
    "        \n",
    "        if \"_logmel\" in base_name:\n",
    "            base_name = base_name.replace(\"_logmel\", \"\")\n",
    "        if \"_flipped\" in base_name:\n",
    "            base_name = base_name.replace(\"_flipped\", \"\")\n",
    "        \n",
    "        parts = base_name.split(\"_\")\n",
    "        if len(parts) >= 2:\n",
    "            audio_filename = f\"{parts[0]}_{parts[1]}.wav\"\n",
    "        else:\n",
    "            audio_filename = f\"{base_name}.wav\"\n",
    "        \n",
    "        if audio_filename in self.features_dict:\n",
    "            return self.features_dict[audio_filename]\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è No features found for {audio_filename}, using zero vector\")\n",
    "            return torch.zeros(len(self.feature_columns), dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        filename = os.path.basename(image_path)\n",
    "        \n",
    "        label = search_in_labels(filename, self.label_dict)\n",
    "        \n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        if self.is_training and label == \"C\":\n",
    "            image = self.c_train_transform(image)\n",
    "        else:\n",
    "            image = self.base_transform(image)\n",
    "\n",
    "        label_num = 1 if label == \"C\" else 0\n",
    "        \n",
    "        physical_features = self.get_physical_features(filename)\n",
    "\n",
    "        return image, physical_features, label_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cfb06b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def create_progressive_undersampling(image_paths, label_dict, stages=[0.4, 0.6, 0.8]):\n",
    "\n",
    "    all_datasets = []\n",
    "    \n",
    "    for stage_ratio in stages:\n",
    "        cold_paths = []\n",
    "        healthy_paths = []\n",
    "        \n",
    "        for path in image_paths:\n",
    "            filename = os.path.basename(path)\n",
    "            label = search_in_labels(filename, label_dict)\n",
    "            \n",
    "            if label == \"C\":\n",
    "                cold_paths.append(path)\n",
    "            elif label == \"NC\":\n",
    "                healthy_paths.append(path)\n",
    "        \n",
    "        target_healthy = int(len(healthy_paths) * stage_ratio)\n",
    "        sampled_healthy = random.sample(healthy_paths, target_healthy)\n",
    "        \n",
    "        stage_paths = cold_paths + sampled_healthy\n",
    "        random.shuffle(stage_paths)\n",
    "        \n",
    "        stage_dataset = SpectrogramDataset(stage_paths, label_dict, physical_features_df, is_training=True)\n",
    "        all_datasets.append(stage_dataset)\n",
    "        \n",
    "        print(f\"üìä Stage {stage_ratio}: {len(cold_paths)} Cold + {len(sampled_healthy)} Healthy\")\n",
    "    \n",
    "    return all_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08b0f763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Collecting image paths...\n",
      "üîç Looking for images in: ../spectrogram_images/log_mel\\train_files\n",
      "üìÅ Found 10475 PNG files in train_files\n",
      "üîç Looking for images in: ../spectrogram_images/log_mel\\devel_files\n",
      "üìÅ Found 10607 PNG files in devel_files\n",
      "üìã After filtering out 'flipped' files: 9596 files\n",
      "üìä Physical features prepared:\n",
      "  Feature dimensions: 75\n",
      "üìä Stage 0.6: 1940 Cold + 5121 Healthy\n",
      "üìä Physical features prepared:\n",
      "  Feature dimensions: 75\n",
      "üìä Stage 0.8: 1940 Cold + 6828 Healthy\n",
      "‚úÖ Created train loader with 8768 samples\n",
      "üìä Physical features prepared:\n",
      "  Feature dimensions: 75\n",
      "üìä Physical features prepared:\n",
      "  Feature dimensions: 75\n",
      "‚úÖ Created devel loader with 9596 samples\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_split = [\"train_files\", \"devel_files\"]\n",
    "img_dir = \"../spectrogram_images/log_mel\"  \n",
    "\n",
    "def collect_image_paths_devel(split_name):\n",
    "        sub_dir = os.path.join(img_dir, split_name)\n",
    "        print(f\"üîç Looking for images in: {sub_dir}\")\n",
    "        \n",
    "        if not os.path.exists(sub_dir):\n",
    "            print(f\"‚ùå Directory does not exist: {sub_dir}\")\n",
    "            return []\n",
    "        \n",
    "        png_files = glob.glob(os.path.join(sub_dir, \"*.png\"))\n",
    "        \n",
    "        filtered_files = [f for f in png_files if \"flipped\" not in os.path.basename(f)]\n",
    "        \n",
    "        print(f\"üìÅ Found {len(png_files)} PNG files in {split_name}\")\n",
    "        print(f\"üìã After filtering out 'flipped' files: {len(filtered_files)} files\")\n",
    "        \n",
    "        return filtered_files\n",
    "\n",
    "def collect_image_paths(split_name):\n",
    "    sub_dir = os.path.join(img_dir, split_name)\n",
    "    print(f\"üîç Looking for images in: {sub_dir}\")\n",
    "    \n",
    "    if not os.path.exists(sub_dir):\n",
    "        print(f\"‚ùå Directory does not exist: {sub_dir}\")\n",
    "        return []\n",
    "    \n",
    "    png_files = glob.glob(os.path.join(sub_dir, \"*.png\"))\n",
    "    print(f\"üìÅ Found {len(png_files)} PNG files in {split_name}\")\n",
    "    \n",
    "    return png_files\n",
    "\n",
    "print(\"üöÄ Collecting image paths...\")\n",
    "train_image_paths = collect_image_paths(\"train_files\")\n",
    "devel_image_paths = collect_image_paths_devel(\"devel_files\")\n",
    "\n",
    "progressive_datasets = create_progressive_undersampling(\n",
    "    train_image_paths, \n",
    "    label_dict, \n",
    "    stages=[0.6, 0.8] \n",
    ")\n",
    "\n",
    "selected_stage = 1 \n",
    "train_dataset = progressive_datasets[selected_stage]\n",
    "train_loader = DataLoader(train_dataset, batch_size = 32, shuffle=True)\n",
    "print(f\"‚úÖ Created train loader with {len(train_dataset)} samples\")\n",
    "\n",
    "devel_dataset = SpectrogramDataset(devel_image_paths, label_dict, physical_features_df, is_training=False)\n",
    "test_dataset = SpectrogramDataset(devel_image_paths, label_dict, physical_features_df, is_training=False) \n",
    "    \n",
    "devel_loader = DataLoader(devel_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(devel_dataset, batch_size=512, shuffle=False)\n",
    "print(f\"‚úÖ Created devel loader with {len(devel_dataset)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcd6370",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "class EnhancedCNNBinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_shape=(3, 300, 100), physical_feature_dim=75, num_classes=1):\n",
    "        super(EnhancedCNNBinaryClassifier, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(input_shape[0], 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((4, 4))\n",
    "        \n",
    "        self.image_feature_dim = 512 * 4 * 4\n",
    "        self.image_reducer = nn.Sequential(\n",
    "            nn.Linear(self.image_feature_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        self.physical_processor = nn.Sequential(\n",
    "            nn.Linear(physical_feature_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 2),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LayerNorm(64)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, image_input, physical_input):\n",
    "        x = F.relu(self.bn1(self.conv1(image_input)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        image_feat = self.image_reducer(x)\n",
    "\n",
    "        phys_feat = self.physical_processor(physical_input)\n",
    "        \n",
    "        combined = torch.cat([image_feat, phys_feat], dim=1)  # [batch, 512]\n",
    "        attention_weights = self.attention(combined)  # [batch, 2]\n",
    "        \n",
    "        fused = (attention_weights[:, 0:1] * image_feat + \n",
    "                attention_weights[:, 1:2] * phys_feat)\n",
    "\n",
    "        embedding = self.projection_head(fused)\n",
    "        embedding = F.normalize(embedding, dim=1)\n",
    "        \n",
    "        logits = self.classifier(fused)\n",
    "        \n",
    "        return embedding, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27cd794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SupervisedContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.05, minority_weight=2.0):\n",
    "        super(SupervisedContrastiveLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.minority_weight = minority_weight\n",
    "\n",
    "    def forward(self, features, labels):\n",
    "        device = features.device\n",
    "        batch_size = features.shape[0]\n",
    "        \n",
    "        labels = labels.contiguous().view(-1, 1)\n",
    "        mask = torch.eq(labels, labels.T).float().to(device)\n",
    "        \n",
    "        anchor_dot_contrast = torch.div(\n",
    "            torch.matmul(features, features.T),\n",
    "            self.temperature\n",
    "        )\n",
    "        \n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "        logits = anchor_dot_contrast - logits_max.detach()\n",
    "        \n",
    "        logits_mask = torch.ones_like(mask) - torch.eye(batch_size, device=device)\n",
    "        mask = mask * logits_mask\n",
    "        \n",
    "        exp_logits = torch.exp(logits) * logits_mask\n",
    "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True) + 1e-12)\n",
    "        \n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / (mask.sum(1) + 1e-12)\n",
    "        \n",
    "        weights = torch.where(labels.squeeze() == 1, self.minority_weight, 1.0).to(device)\n",
    "        loss = -(weights * mean_log_prob_pos).mean()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, classification_loss, contrastive_loss, alpha=0.3):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.classification_loss = classification_loss\n",
    "        self.contrastive_loss = contrastive_loss\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def forward(self, logits, embeddings, labels):\n",
    "        cls_loss = self.classification_loss(logits.squeeze(), labels.float())\n",
    "        cont_loss = self.contrastive_loss(embeddings, labels)\n",
    "        total_loss = cls_loss + self.alpha * cont_loss\n",
    "        return total_loss, cls_loss, cont_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6c8acfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = EnhancedCNNBinaryClassifier(\n",
    "    input_shape=(3, 210, 70), \n",
    "    physical_feature_dim=len(train_dataset.feature_columns)\n",
    ").to(device)\n",
    "classification_loss = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(6.0).to(device))\n",
    "contrastive_loss = SupervisedContrastiveLoss(temperature=0.05, minority_weight=2.0)\n",
    "criterion = CombinedLoss(\n",
    "    classification_loss=classification_loss,\n",
    "    contrastive_loss=contrastive_loss,\n",
    "    alpha=0.3\n",
    ")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5, weight_decay=1e-6)\n",
    "\n",
    "num_epochs = 100\n",
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9dc9026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 274/274 [01:10<00:00,  3.88it/s, total_loss=3.1334, cls_loss=1.6805, cont_loss=4.8432]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 1 Losses:\n",
      "  Total: 3.0151, Classification: 1.5003, Contrastive: 5.0496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [01:04<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1] Summary:\n",
      "  üìà Training   - Loss: 3.0151, Acc: 0.6548, UAR: 0.5427\n",
      "  üìä Validation - Loss: 2.4275, Acc: 0.7258, UAR: 0.6395\n",
      "  üì¶ Processed  - Train: 8768 samples, Val: 9596 samples\n",
      "üåü New best UAR: 0.6395, saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 274/274 [01:09<00:00,  3.97it/s, total_loss=2.3784, cls_loss=1.1393, cont_loss=4.1305]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 2 Losses:\n",
      "  Total: 2.6433, Classification: 1.3441, Contrastive: 4.3307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [01:03<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [2] Summary:\n",
      "  üìà Training   - Loss: 2.6433, Acc: 0.5993, UAR: 0.6193\n",
      "  üìä Validation - Loss: 2.3758, Acc: 0.6513, UAR: 0.6328\n",
      "  üì¶ Processed  - Train: 8768 samples, Val: 9596 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 274/274 [01:08<00:00,  3.97it/s, total_loss=1.9833, cls_loss=0.7615, cont_loss=4.0728]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 3 Losses:\n",
      "  Total: 2.4315, Classification: 1.1629, Contrastive: 4.2287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [01:03<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [3] Summary:\n",
      "  üìà Training   - Loss: 2.4315, Acc: 0.6169, UAR: 0.6793\n",
      "  üìä Validation - Loss: 2.3782, Acc: 0.8367, UAR: 0.5475\n",
      "  üì¶ Processed  - Train: 8768 samples, Val: 9596 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 274/274 [01:09<00:00,  3.96it/s, total_loss=1.6132, cls_loss=0.5138, cont_loss=3.6649]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 4 Losses:\n",
      "  Total: 2.0000, Classification: 0.7961, Contrastive: 4.0131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [01:03<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [4] Summary:\n",
      "  üìà Training   - Loss: 2.0000, Acc: 0.7468, UAR: 0.8074\n",
      "  üìä Validation - Loss: 2.9014, Acc: 0.4438, UAR: 0.5255\n",
      "  üì¶ Processed  - Train: 8768 samples, Val: 9596 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 274/274 [01:09<00:00,  3.97it/s, total_loss=1.5548, cls_loss=0.4965, cont_loss=3.5276]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 5 Losses:\n",
      "  Total: 1.5135, Classification: 0.4287, Contrastive: 3.6159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [01:03<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [5] Summary:\n",
      "  üìà Training   - Loss: 1.5135, Acc: 0.8985, UAR: 0.9306\n",
      "  üìä Validation - Loss: 2.5207, Acc: 0.8474, UAR: 0.5299\n",
      "  üì¶ Processed  - Train: 8768 samples, Val: 9596 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 274/274 [01:08<00:00,  4.00it/s, total_loss=1.3295, cls_loss=0.3273, cont_loss=3.3407]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 6 Losses:\n",
      "  Total: 1.2986, Classification: 0.2618, Contrastive: 3.4562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [01:02<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [6] Summary:\n",
      "  üìà Training   - Loss: 1.2986, Acc: 0.9528, UAR: 0.9671\n",
      "  üìä Validation - Loss: 2.7829, Acc: 0.8780, UAR: 0.5186\n",
      "  üì¶ Processed  - Train: 8768 samples, Val: 9596 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 274/274 [01:08<00:00,  3.98it/s, total_loss=1.0618, cls_loss=0.0712, cont_loss=3.3018]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 7 Losses:\n",
      "  Total: 1.2112, Classification: 0.1825, Contrastive: 3.4290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [01:02<00:00,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [7] Summary:\n",
      "  üìà Training   - Loss: 1.2112, Acc: 0.9689, UAR: 0.9778\n",
      "  üìä Validation - Loss: 2.9361, Acc: 0.8820, UAR: 0.5187\n",
      "  üì¶ Processed  - Train: 8768 samples, Val: 9596 samples\n",
      "No improvement in UAR for 6 epochs, early stopping...\n",
      "\n",
      "üéâ Training complete in 15.46 min\n",
      "Best Validation UAR: 0.6395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_uar = 0.0\n",
    "patience = 6\n",
    "patience_counter = 0\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "start_time = time.time()\n",
    "early_stop_counter  = 0\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_cls_loss = 0.0\n",
    "    running_cont_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\")\n",
    "\n",
    "    for image, physical_features, label_num in progress_bar:\n",
    "        batch_X = image.to(device)\n",
    "        batch_y = label_num.to(device)\n",
    "        physical_features = physical_features.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        embeddings, logits = model(batch_X, physical_features)\n",
    "        \n",
    "        total_loss, cls_loss, cont_loss = criterion(logits, embeddings, batch_y)\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = (torch.sigmoid(logits.squeeze()) > threshold).long()\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "        running_loss += total_loss.item()\n",
    "        running_cls_loss += cls_loss.item()\n",
    "        running_cont_loss += cont_loss.item()\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            'total_loss': f'{total_loss.item():.4f}',\n",
    "            'cls_loss': f'{cls_loss.item():.4f}',\n",
    "            'cont_loss': f'{cont_loss.item():.4f}'\n",
    "        })\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_cls_loss = running_cls_loss / len(train_loader)\n",
    "    epoch_cont_loss = running_cont_loss / len(train_loader)\n",
    "    \n",
    "    print(f\"üìä Epoch {epoch+1} Losses:\")\n",
    "    print(f\"  Total: {epoch_loss:.4f}, Classification: {epoch_cls_loss:.4f}, Contrastive: {epoch_cont_loss:.4f}\")\n",
    "    \n",
    "    train_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    train_uar = recall_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    model.eval()\n",
    "    val_loss, val_preds, val_labels = 0.0, [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, physical_features, label_num in tqdm(devel_loader, desc=\"Validating\"):\n",
    "            batch_X = image.to(device)\n",
    "            physical_features = physical_features.to(device)\n",
    "            batch_y = label_num.to(device)\n",
    "\n",
    "            embeddings, logits = model(batch_X, physical_features)\n",
    "            \n",
    "            total_loss, cls_loss, cont_loss = criterion(logits, embeddings, batch_y)\n",
    "            val_loss += total_loss.item()\n",
    "\n",
    "            preds = (torch.sigmoid(logits.squeeze()) > threshold).long()\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(devel_loader)\n",
    "    validation_losses.append(avg_val_loss)\n",
    "\n",
    "    val_accuracy = accuracy_score(val_labels, val_preds)\n",
    "    val_uar = recall_score(val_labels, val_preds, average='macro')\n",
    "    \n",
    "\n",
    "    print(f\"\\nEpoch [{epoch+1}] Summary:\")\n",
    "    print(f\"  üìà Training   - Loss: {epoch_loss:.4f}, Acc: {train_accuracy:.4f}, UAR: {train_uar:.4f}\")\n",
    "    print(f\"  üìä Validation - Loss: {avg_val_loss:.4f}, Acc: {val_accuracy:.4f}, UAR: {val_uar:.4f}\")\n",
    "    print(f\"  üì¶ Processed  - Train: {len(all_labels)} samples, Val: {len(val_labels)} samples\")\n",
    "\n",
    "    if val_uar > best_uar:\n",
    "        best_uar = val_uar\n",
    "        early_stop_counter = 0\n",
    "        torch.save(model.state_dict(), \"best_cv_fusion.pth\")\n",
    "        print(f\"üåü New best UAR: {best_uar:.4f}, saving model...\")\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"No improvement in UAR for {patience} epochs, early stopping...\")\n",
    "            break\n",
    "\n",
    "print(f\"\\nüéâ Training complete in {(time.time() - start_time)/60:.2f} min\")\n",
    "print(f\"Best Validation UAR: {best_uar:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
