{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8869d322",
   "metadata": {},
   "source": [
    "# Lora Fine-tuning on WAV2WEC2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577f5c4c",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78701ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading labels: 100%|██████████| 19101/19101 [00:00<00:00, 3820841.32it/s]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "label_dict = {}\n",
    "with open(\"../ComParE2017_Cold_4students/lab/ComParE2017_Cold.tsv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f, delimiter=\"\\t\")\n",
    "    rows = list(reader)\n",
    "    for row in tqdm(rows, desc=\"Loading labels\"):\n",
    "        label_dict[row[\"file_name\"]] = row[\"Cold (upper respiratory tract infection)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eae144b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_in_ground_truth(file_id: str, label_dict: dict) -> str:\n",
    "    wav_name = file_id + \".wav\"\n",
    "    return label_dict.get(wav_name, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e071fc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8df992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ContrastiveEmbeddingExtractor(nn.Module):\n",
    "    def __init__(self, input_dim=3072, projection_dim=256, hidden_dim=512):\n",
    "        super(ContrastiveEmbeddingExtractor, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, projection_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(projection_dim, projection_dim),\n",
    "            L2Norm(dim=1)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, return_features=False):\n",
    "        features = self.encoder(x)\n",
    "        \n",
    "        logits = self.classifier(features)\n",
    "        \n",
    "        if return_features:\n",
    "            projections = self.projection_head(features)\n",
    "            return logits.squeeze(), projections, features\n",
    "        else:\n",
    "            return logits.squeeze()\n",
    "\n",
    "class L2Norm(nn.Module):\n",
    "    def __init__(self, dim=1):\n",
    "        super(L2Norm, self).__init__()\n",
    "        self.dim = dim\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return F.normalize(x, p=2, dim=self.dim)\n",
    "    \n",
    "class SupervisedContrastiveLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, temperature=0.1, minority_weight=2.0):\n",
    "        super(SupervisedContrastiveLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.minority_weight = minority_weight\n",
    "        \n",
    "    def forward(self, projections, labels):\n",
    "        device = projections.device\n",
    "        batch_size = projections.shape[0]\n",
    "        \n",
    "        similarity_matrix = torch.matmul(projections, projections.T) / self.temperature\n",
    "        \n",
    "        labels = labels.unsqueeze(1)\n",
    "        mask = torch.eq(labels, labels.T).float().to(device)\n",
    "        \n",
    "        mask = mask - torch.eye(batch_size).to(device)\n",
    "        \n",
    "        exp_sim = torch.exp(similarity_matrix)\n",
    "        \n",
    "        pos_sim = exp_sim * mask\n",
    "        \n",
    "        neg_mask = torch.ones_like(mask) - torch.eye(batch_size).to(device)\n",
    "        all_sim = exp_sim * neg_mask\n",
    "        \n",
    "        losses = []\n",
    "        for i in range(batch_size):\n",
    "            if mask[i].sum() > 0:  \n",
    "                pos_sum = pos_sim[i].sum()\n",
    "                neg_sum = all_sim[i].sum()\n",
    "                \n",
    "                if neg_sum > 0:\n",
    "                    loss_i = -torch.log(pos_sum / neg_sum)\n",
    "                    \n",
    "                    if labels[i] == 1:  \n",
    "                        loss_i = loss_i * self.minority_weight\n",
    "                    \n",
    "                    losses.append(loss_i)\n",
    "        \n",
    "        if len(losses) > 0:\n",
    "            return torch.stack(losses).mean()\n",
    "        else:\n",
    "            return torch.tensor(0.0).to(device)\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, classification_loss, contrastive_loss, alpha=0.3):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.classification_loss = classification_loss\n",
    "        self.contrastive_loss = contrastive_loss\n",
    "        self.alpha = alpha  \n",
    "        \n",
    "    def forward(self, logits, projections, labels):\n",
    "        cls_loss = self.classification_loss(logits, labels.float())\n",
    "        \n",
    "        cont_loss = self.contrastive_loss(projections, labels)\n",
    "        \n",
    "        total_loss = (1 - self.alpha) * cls_loss + self.alpha * cont_loss\n",
    "        \n",
    "        return total_loss, cls_loss, cont_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67cd617",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntegratedModel(nn.Module):\n",
    "    def __init__(self, wav2vec2_model, processor, downstream_model):\n",
    "        super(IntegratedModel, self).__init__()\n",
    "        self.wav2vec2_model = wav2vec2_model\n",
    "        self.processor = processor\n",
    "        self.downstream_model = downstream_model\n",
    "        \n",
    "    def forward(self, waveforms, return_features=False):\n",
    "        device = waveforms.device\n",
    "        \n",
    "        waveforms_np = waveforms.cpu().numpy()\n",
    "        \n",
    "        inputs = self.processor(\n",
    "            waveforms_np, \n",
    "            sampling_rate=16000, \n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=160000\n",
    "        )\n",
    "        \n",
    "        input_values = inputs['input_values'].to(device)\n",
    "        attention_mask = inputs.get('attention_mask', None)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.to(device)\n",
    " \n",
    "        out = self.wav2vec2_model(\n",
    "                input_values, \n",
    "                attention_mask=attention_mask,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "        hs = out.hidden_states\n",
    "        first_k = hs[1:3]\n",
    "        \n",
    "        pooled = []\n",
    "        for layer in first_k:\n",
    "            m = layer.mean(dim=1)\n",
    "            mx = layer.max(dim=1).values\n",
    "            pooled.extend([m, mx])\n",
    "        \n",
    "        embedding = torch.cat(pooled, dim=-1)\n",
    "\n",
    "        # embedding = out.last_hidden_state.mean(dim=1)\n",
    "        result = self.downstream_model(embedding, return_features=True)\n",
    "        if isinstance(result, tuple) and len(result) == 3:\n",
    "                return result\n",
    "        else:\n",
    "            logits = result\n",
    "            return logits, embedding, embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56a091be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wav2Vec2 Model Structure:\n",
      "  encoder.layers.0.attention: <class 'transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2SdpaAttention'>\n",
      "  encoder.layers.0.attention.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.0.attention.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.0.attention.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.0.attention.out_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.1.attention: <class 'transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2SdpaAttention'>\n",
      "  encoder.layers.1.attention.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.1.attention.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.1.attention.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.1.attention.out_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.2.attention: <class 'transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2SdpaAttention'>\n",
      "  encoder.layers.2.attention.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.2.attention.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.2.attention.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.2.attention.out_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.3.attention: <class 'transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2SdpaAttention'>\n",
      "  encoder.layers.3.attention.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.3.attention.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.3.attention.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.3.attention.out_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.4.attention: <class 'transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2SdpaAttention'>\n",
      "  encoder.layers.4.attention.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.4.attention.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.4.attention.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.4.attention.out_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.5.attention: <class 'transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2SdpaAttention'>\n",
      "  encoder.layers.5.attention.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.5.attention.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.5.attention.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.5.attention.out_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.6.attention: <class 'transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2SdpaAttention'>\n",
      "  encoder.layers.6.attention.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.6.attention.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.6.attention.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.6.attention.out_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.7.attention: <class 'transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2SdpaAttention'>\n",
      "  encoder.layers.7.attention.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.7.attention.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.7.attention.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.7.attention.out_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.8.attention: <class 'transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2SdpaAttention'>\n",
      "  encoder.layers.8.attention.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.8.attention.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.8.attention.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.8.attention.out_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.9.attention: <class 'transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2SdpaAttention'>\n",
      "  encoder.layers.9.attention.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.9.attention.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.9.attention.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.9.attention.out_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.10.attention: <class 'transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2SdpaAttention'>\n",
      "  encoder.layers.10.attention.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.10.attention.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.10.attention.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.10.attention.out_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.11.attention: <class 'transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2SdpaAttention'>\n",
      "  encoder.layers.11.attention.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.11.attention.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.11.attention.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "  encoder.layers.11.attention.out_proj: <class 'torch.nn.modules.linear.Linear'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model = AutoModel.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "print(\"Wav2Vec2 Model Structure:\")\n",
    "for name, module in model.named_modules():\n",
    "    if 'attention' in name or 'query' in name or 'key' in name or 'value' in name:\n",
    "        print(f\"  {name}: {type(module)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7283bbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  \n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"encoder.layers.10.attention.q_proj\",\n",
    "                    \"encoder.layers.10.attention.v_proj\",\n",
    "                    \"encoder.layers.10.attention.k_proj\",\n",
    "                    \"encoder.layers.11.attention.q_proj\",\n",
    "                    \"encoder.layers.11.attention.v_proj\",\n",
    "                    ],\n",
    "    base_model_name_or_path=\"facebook/wav2vec2-base-960h\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9670b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "upstream_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\").to(\"cuda\")\n",
    "lora_model = get_peft_model(upstream_model, lora_config)\n",
    "\n",
    "model_downstream = ContrastiveEmbeddingExtractor(input_dim=3072,projection_dim=512,hidden_dim=256).to(device)\n",
    "model_downstream.load_state_dict(torch.load(\"best_contrastive_embedding_model_first.pth\", map_location=device))\n",
    "\n",
    "model = IntegratedModel(lora_model, processor, model_downstream).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf657b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wav2vec2_model.base_model.model.masked_spec_embed: requires_grad=False\n",
      "wav2vec2_model.base_model.model.feature_extractor.conv_layers.0.conv.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.feature_extractor.conv_layers.0.layer_norm.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.feature_extractor.conv_layers.0.layer_norm.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.feature_extractor.conv_layers.1.conv.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.feature_extractor.conv_layers.2.conv.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.feature_extractor.conv_layers.3.conv.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.feature_extractor.conv_layers.4.conv.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.feature_extractor.conv_layers.5.conv.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.feature_extractor.conv_layers.6.conv.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.feature_projection.layer_norm.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.feature_projection.layer_norm.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.feature_projection.projection.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.feature_projection.projection.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.pos_conv_embed.conv.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.pos_conv_embed.conv.parametrizations.weight.original0: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.pos_conv_embed.conv.parametrizations.weight.original1: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layer_norm.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layer_norm.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.0.attention.k_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.0.attention.k_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.0.attention.v_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.0.attention.v_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.0.attention.q_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.0.attention.q_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.0.attention.out_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.0.attention.out_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.0.layer_norm.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.0.layer_norm.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.0.feed_forward.intermediate_dense.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.0.feed_forward.output_dense.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.0.feed_forward.output_dense.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.0.final_layer_norm.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.0.final_layer_norm.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.1.attention.k_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.1.attention.k_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.1.attention.v_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.1.attention.v_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.1.attention.q_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.1.attention.q_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.1.attention.out_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.1.attention.out_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.1.layer_norm.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.1.layer_norm.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.1.feed_forward.intermediate_dense.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.1.feed_forward.output_dense.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.1.feed_forward.output_dense.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.1.final_layer_norm.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.1.final_layer_norm.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.2.attention.k_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.2.attention.k_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.2.attention.v_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.2.attention.v_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.2.attention.q_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.2.attention.q_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.2.attention.out_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.2.attention.out_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.2.layer_norm.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.2.layer_norm.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.2.feed_forward.intermediate_dense.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.2.feed_forward.output_dense.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.2.feed_forward.output_dense.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.2.final_layer_norm.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.2.final_layer_norm.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.3.attention.k_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.3.attention.k_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.3.attention.v_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.3.attention.v_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.3.attention.q_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.3.attention.q_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.3.attention.out_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.3.attention.out_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.3.layer_norm.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.3.layer_norm.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.3.feed_forward.intermediate_dense.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.3.feed_forward.output_dense.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.3.feed_forward.output_dense.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.3.final_layer_norm.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.3.final_layer_norm.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.4.attention.k_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.4.attention.k_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.4.attention.v_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.4.attention.v_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.4.attention.q_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.4.attention.q_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.4.attention.out_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.4.attention.out_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.4.layer_norm.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.4.layer_norm.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.4.feed_forward.intermediate_dense.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.4.feed_forward.output_dense.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.4.feed_forward.output_dense.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.4.final_layer_norm.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.4.final_layer_norm.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.5.attention.k_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.5.attention.k_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.5.attention.v_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.5.attention.v_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.5.attention.q_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.5.attention.q_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.5.attention.out_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.5.attention.out_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.5.layer_norm.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.5.layer_norm.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.5.feed_forward.intermediate_dense.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.5.feed_forward.output_dense.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.5.feed_forward.output_dense.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.5.final_layer_norm.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.5.final_layer_norm.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.6.attention.k_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.6.attention.k_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.6.attention.v_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.6.attention.v_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.6.attention.q_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.6.attention.q_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.6.attention.out_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.6.attention.out_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.6.layer_norm.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.6.layer_norm.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.6.feed_forward.intermediate_dense.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.6.feed_forward.intermediate_dense.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.6.feed_forward.output_dense.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.6.feed_forward.output_dense.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.6.final_layer_norm.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.6.final_layer_norm.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.7.attention.k_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.7.attention.k_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.7.attention.v_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.7.attention.v_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.7.attention.q_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.7.attention.q_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.7.attention.out_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.7.attention.out_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.7.layer_norm.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.7.layer_norm.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.7.feed_forward.intermediate_dense.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.7.feed_forward.intermediate_dense.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.7.feed_forward.output_dense.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.7.feed_forward.output_dense.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.7.final_layer_norm.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.7.final_layer_norm.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.8.attention.k_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.8.attention.k_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.8.attention.v_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.8.attention.v_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.8.attention.q_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.8.attention.q_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.8.attention.out_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.8.attention.out_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.8.layer_norm.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.8.layer_norm.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.8.feed_forward.intermediate_dense.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.8.feed_forward.intermediate_dense.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.8.feed_forward.output_dense.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.8.feed_forward.output_dense.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.8.final_layer_norm.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.8.final_layer_norm.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.9.attention.k_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.9.attention.k_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.9.attention.v_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.9.attention.v_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.9.attention.q_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.9.attention.q_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.9.attention.out_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.9.attention.out_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.9.layer_norm.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.9.layer_norm.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.9.feed_forward.intermediate_dense.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.9.feed_forward.intermediate_dense.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.9.feed_forward.output_dense.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.9.feed_forward.output_dense.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.9.final_layer_norm.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.9.final_layer_norm.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.10.attention.k_proj.base_layer.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.10.attention.k_proj.base_layer.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.10.attention.k_proj.lora_A.default.weight: requires_grad=True\n",
      "wav2vec2_model.base_model.model.encoder.layers.10.attention.k_proj.lora_B.default.weight: requires_grad=True\n",
      "wav2vec2_model.base_model.model.encoder.layers.10.attention.v_proj.base_layer.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.10.attention.v_proj.base_layer.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.10.attention.v_proj.lora_A.default.weight: requires_grad=True\n",
      "wav2vec2_model.base_model.model.encoder.layers.10.attention.v_proj.lora_B.default.weight: requires_grad=True\n",
      "wav2vec2_model.base_model.model.encoder.layers.10.attention.q_proj.base_layer.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.10.attention.q_proj.base_layer.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.10.attention.q_proj.lora_A.default.weight: requires_grad=True\n",
      "wav2vec2_model.base_model.model.encoder.layers.10.attention.q_proj.lora_B.default.weight: requires_grad=True\n",
      "wav2vec2_model.base_model.model.encoder.layers.10.attention.out_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.10.attention.out_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.10.layer_norm.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.10.layer_norm.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.10.feed_forward.intermediate_dense.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.10.feed_forward.intermediate_dense.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.10.feed_forward.output_dense.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.10.feed_forward.output_dense.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.10.final_layer_norm.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.10.final_layer_norm.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.11.attention.k_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.11.attention.k_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.11.attention.v_proj.base_layer.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.11.attention.v_proj.base_layer.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.11.attention.v_proj.lora_A.default.weight: requires_grad=True\n",
      "wav2vec2_model.base_model.model.encoder.layers.11.attention.v_proj.lora_B.default.weight: requires_grad=True\n",
      "wav2vec2_model.base_model.model.encoder.layers.11.attention.q_proj.base_layer.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.11.attention.q_proj.base_layer.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.11.attention.q_proj.lora_A.default.weight: requires_grad=True\n",
      "wav2vec2_model.base_model.model.encoder.layers.11.attention.q_proj.lora_B.default.weight: requires_grad=True\n",
      "wav2vec2_model.base_model.model.encoder.layers.11.attention.out_proj.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.11.attention.out_proj.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.11.layer_norm.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.11.layer_norm.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.11.feed_forward.intermediate_dense.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.11.feed_forward.intermediate_dense.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.11.feed_forward.output_dense.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.11.feed_forward.output_dense.bias: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.11.final_layer_norm.weight: requires_grad=False\n",
      "wav2vec2_model.base_model.model.encoder.layers.11.final_layer_norm.bias: requires_grad=False\n",
      "downstream_model.encoder.0.weight: requires_grad=True\n",
      "downstream_model.encoder.0.bias: requires_grad=True\n",
      "downstream_model.encoder.3.weight: requires_grad=True\n",
      "downstream_model.encoder.3.bias: requires_grad=True\n",
      "downstream_model.projection_head.0.weight: requires_grad=True\n",
      "downstream_model.projection_head.0.bias: requires_grad=True\n",
      "downstream_model.projection_head.2.weight: requires_grad=True\n",
      "downstream_model.projection_head.2.bias: requires_grad=True\n",
      "downstream_model.classifier.0.weight: requires_grad=True\n",
      "downstream_model.classifier.0.bias: requires_grad=True\n",
      "downstream_model.classifier.3.weight: requires_grad=True\n",
      "downstream_model.classifier.3.bias: requires_grad=True\n",
      "downstream_model.classifier.6.weight: requires_grad=True\n",
      "downstream_model.classifier.6.bias: requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for name, param in model.named_parameters():\n",
    "    if 'lora' in name:\n",
    "        param.requires_grad = True\n",
    "    if'downstream_model' in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: requires_grad={param.requires_grad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77388bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "input_root = \"../ComParE2017_Cold_4students/wav/\"\n",
    "data_dir = \"processed_files\"\n",
    "data_split = [\"train_files\", \"devel_files\"]\n",
    "\n",
    "train_dir = os.path.join(input_root, data_split[0], data_dir)\n",
    "devel_dir = os.path.join(input_root, data_split[1], data_dir)\n",
    "\n",
    "train_files = [f for f in os.listdir(train_dir) if f.endswith('.wav')]\n",
    "devel_files = [f for f in os.listdir(devel_dir) if f.endswith('.wav')]\n",
    "\n",
    "class Wav2Vec2Dataset(Dataset):\n",
    "    def __init__(self, file_list, label_dict, split, is_training = False, input_root=\"../ComParE2017_Cold_4students/wav/\", max_length=160000):\n",
    "        self.file_list = file_list\n",
    "        self.label_dict = label_dict\n",
    "        self.input_root = input_root \n",
    "        self.split = split\n",
    "        self.max_length = max_length\n",
    "        self.is_training = is_training\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_list[idx]\n",
    "        file_path = os.path.join(self.input_root, self.split, \"processed_files\", file_name)\n",
    "        \n",
    "        label = search_in_ground_truth(file_name[:-4], self.label_dict)\n",
    "        \n",
    "        try:\n",
    "            waveform, sr = librosa.load(file_path, sr=16000)\n",
    "            \n",
    "            if self.is_training and label == \"C\":\n",
    "                try:\n",
    "                    file_path_additional = os.path.join(self.input_root, self.split, file_name)\n",
    "                    if os.path.exists(file_path_additional):\n",
    "                        additional_waveform, _ = librosa.load(file_path_additional, sr=16000)\n",
    "                        \n",
    "                        if np.random.random() > 0.5:\n",
    "                            waveform = additional_waveform\n",
    "                            #print(f\"Using additional file for {file_name}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Could not load additional file {file_path_additional}: {e}\")\n",
    "            \n",
    "            if len(waveform) > self.max_length:\n",
    "                waveform = waveform[:self.max_length]\n",
    "            elif len(waveform) < self.max_length:\n",
    "                padding = self.max_length - len(waveform)\n",
    "                waveform = np.pad(waveform, (0, padding), mode='constant', constant_values=0)\n",
    "            \n",
    "            if len(waveform) == 0:\n",
    "                raise ValueError(\"Empty audio file\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "            waveform = np.zeros(self.max_length)\n",
    "        \n",
    "        label_tensor = torch.tensor(1 if label == \"C\" else 0)\n",
    "        \n",
    "        return torch.tensor(waveform, dtype=torch.float32), label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17ac2c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Train Set - First 5 samples ===\n",
      "Sample 1:\n",
      "  File: train_0001.wav\n",
      "  Input shape: torch.Size([160000])\n",
      "  Label: 1 (Cold)\n",
      "\n",
      "Sample 2:\n",
      "  File: train_0002.wav\n",
      "  Input shape: torch.Size([160000])\n",
      "  Label: 0 (Healthy)\n",
      "\n",
      "Sample 3:\n",
      "  File: train_0003.wav\n",
      "  Input shape: torch.Size([160000])\n",
      "  Label: 0 (Healthy)\n",
      "\n",
      "Sample 4:\n",
      "  File: train_0004.wav\n",
      "  Input shape: torch.Size([160000])\n",
      "  Label: 1 (Cold)\n",
      "\n",
      "Sample 5:\n",
      "  File: train_0005.wav\n",
      "  Input shape: torch.Size([160000])\n",
      "  Label: 0 (Healthy)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_set = Wav2Vec2Dataset(train_files, label_dict, \"train_files\", is_training=True, input_root= \"../ComParE2017_Cold_4students/wav/\")\n",
    "devel_set = Wav2Vec2Dataset(devel_files, label_dict, \"devel_files\", input_root)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=4, shuffle=True)\n",
    "devel_loader = DataLoader(devel_set, batch_size=4, shuffle=False)\n",
    "\n",
    "print(\"=== Train Set - First 5 samples ===\")\n",
    "for i in range(min(5, len(train_set))):\n",
    "    input_values, label = train_set[i]\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"  File: {train_set.file_list[i] if hasattr(train_set, 'file_list') else 'unknown'}\")\n",
    "    print(f\"  Input shape: {input_values.shape}\")\n",
    "    print(f\"  Label: {label.item()} ({'Cold' if label.item() == 1 else 'Healthy'})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54408a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs= 50\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-7, weight_decay=1e-8)\n",
    "classification_loss = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(5).to(device))\n",
    "contrastive_loss = SupervisedContrastiveLoss(temperature=0.05, minority_weight=2.0)\n",
    "criterion = CombinedLoss(\n",
    "    classification_loss=classification_loss,\n",
    "    contrastive_loss=contrastive_loss,\n",
    "    alpha=0.4\n",
    ")\n",
    "threshold = 0.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f713d970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wav2vec2_model.base_model.model.encoder.layers.10.attention.k_proj.lora_A.default.weight True NO GRAD\n",
      "wav2vec2_model.base_model.model.encoder.layers.10.attention.k_proj.lora_B.default.weight True NO GRAD\n",
      "wav2vec2_model.base_model.model.encoder.layers.10.attention.v_proj.lora_A.default.weight True NO GRAD\n",
      "wav2vec2_model.base_model.model.encoder.layers.10.attention.v_proj.lora_B.default.weight True NO GRAD\n",
      "wav2vec2_model.base_model.model.encoder.layers.10.attention.q_proj.lora_A.default.weight True NO GRAD\n",
      "wav2vec2_model.base_model.model.encoder.layers.10.attention.q_proj.lora_B.default.weight True NO GRAD\n",
      "wav2vec2_model.base_model.model.encoder.layers.11.attention.v_proj.lora_A.default.weight True NO GRAD\n",
      "wav2vec2_model.base_model.model.encoder.layers.11.attention.v_proj.lora_B.default.weight True NO GRAD\n",
      "wav2vec2_model.base_model.model.encoder.layers.11.attention.q_proj.lora_A.default.weight True NO GRAD\n",
      "wav2vec2_model.base_model.model.encoder.layers.11.attention.q_proj.lora_B.default.weight True NO GRAD\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if 'lora' in name:\n",
    "        print(name, param.requires_grad, param.grad.norm() if param.grad is not None else \"NO GRAD\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0f7d015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 1468417\n"
     ]
    }
   ],
   "source": [
    "#print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "710e7566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Detailed LoRA Parameter Check:\n",
      "LoRA param: wav2vec2_model.base_model.model.encoder.layers.10.attention.k_proj.lora_A.default.weight\n",
      "  - requires_grad: True\n",
      "  - shape: torch.Size([16, 768])\n",
      "  - device: cuda:0\n",
      "  - dtype: torch.float32\n",
      "LoRA param: wav2vec2_model.base_model.model.encoder.layers.10.attention.k_proj.lora_B.default.weight\n",
      "  - requires_grad: True\n",
      "  - shape: torch.Size([768, 16])\n",
      "  - device: cuda:0\n",
      "  - dtype: torch.float32\n",
      "LoRA param: wav2vec2_model.base_model.model.encoder.layers.10.attention.v_proj.lora_A.default.weight\n",
      "  - requires_grad: True\n",
      "  - shape: torch.Size([16, 768])\n",
      "  - device: cuda:0\n",
      "  - dtype: torch.float32\n",
      "LoRA param: wav2vec2_model.base_model.model.encoder.layers.10.attention.v_proj.lora_B.default.weight\n",
      "  - requires_grad: True\n",
      "  - shape: torch.Size([768, 16])\n",
      "  - device: cuda:0\n",
      "  - dtype: torch.float32\n",
      "LoRA param: wav2vec2_model.base_model.model.encoder.layers.10.attention.q_proj.lora_A.default.weight\n",
      "  - requires_grad: True\n",
      "  - shape: torch.Size([16, 768])\n",
      "  - device: cuda:0\n",
      "  - dtype: torch.float32\n",
      "LoRA param: wav2vec2_model.base_model.model.encoder.layers.10.attention.q_proj.lora_B.default.weight\n",
      "  - requires_grad: True\n",
      "  - shape: torch.Size([768, 16])\n",
      "  - device: cuda:0\n",
      "  - dtype: torch.float32\n",
      "LoRA param: wav2vec2_model.base_model.model.encoder.layers.11.attention.v_proj.lora_A.default.weight\n",
      "  - requires_grad: True\n",
      "  - shape: torch.Size([16, 768])\n",
      "  - device: cuda:0\n",
      "  - dtype: torch.float32\n",
      "LoRA param: wav2vec2_model.base_model.model.encoder.layers.11.attention.v_proj.lora_B.default.weight\n",
      "  - requires_grad: True\n",
      "  - shape: torch.Size([768, 16])\n",
      "  - device: cuda:0\n",
      "  - dtype: torch.float32\n",
      "LoRA param: wav2vec2_model.base_model.model.encoder.layers.11.attention.q_proj.lora_A.default.weight\n",
      "  - requires_grad: True\n",
      "  - shape: torch.Size([16, 768])\n",
      "  - device: cuda:0\n",
      "  - dtype: torch.float32\n",
      "LoRA param: wav2vec2_model.base_model.model.encoder.layers.11.attention.q_proj.lora_B.default.weight\n",
      "  - requires_grad: True\n",
      "  - shape: torch.Size([768, 16])\n",
      "  - device: cuda:0\n",
      "  - dtype: torch.float32\n",
      "\n",
      "📊 Summary:\n",
      "  Total parameters: 235\n",
      "  LoRA parameters: 10\n",
      "  Trainable LoRA parameters: 10\n",
      "  PEFT config: {'default': LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='facebook/wav2vec2-base-960h', revision=None, inference_mode=False, r=16, target_modules={'encoder.layers.10.attention.k_proj', 'encoder.layers.11.attention.q_proj', 'encoder.layers.11.attention.v_proj', 'encoder.layers.10.attention.v_proj', 'encoder.layers.10.attention.q_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)}\n",
      "  Active adapters: ['default']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def detailed_lora_check(model):\n",
    "    print(\"🔍 Detailed LoRA Parameter Check:\")\n",
    "    \n",
    "    total_params = 0\n",
    "    lora_params = 0\n",
    "    trainable_params = 0\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        total_params += 1\n",
    "        \n",
    "        if 'lora' in name.lower():\n",
    "            lora_params += 1\n",
    "            print(f\"LoRA param: {name}\")\n",
    "            print(f\"  - requires_grad: {param.requires_grad}\")\n",
    "            print(f\"  - shape: {param.shape}\")\n",
    "            print(f\"  - device: {param.device}\")\n",
    "            print(f\"  - dtype: {param.dtype}\")\n",
    "            \n",
    "            if param.requires_grad:\n",
    "                trainable_params += 1\n",
    "    \n",
    "    print(f\"\\n📊 Summary:\")\n",
    "    print(f\"  Total parameters: {total_params}\")\n",
    "    print(f\"  LoRA parameters: {lora_params}\")\n",
    "    print(f\"  Trainable LoRA parameters: {trainable_params}\")\n",
    "    \n",
    "    if hasattr(model.wav2vec2_model, 'peft_config'):\n",
    "        print(f\"  PEFT config: {model.wav2vec2_model.peft_config}\")\n",
    "    \n",
    "    if hasattr(model.wav2vec2_model, 'active_adapters'):\n",
    "        print(f\"  Active adapters: {model.wav2vec2_model.active_adapters}\")\n",
    "    \n",
    "    return lora_params, trainable_params\n",
    "\n",
    "detailed_lora_check(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9b854b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Creating simplest model...\n",
      "✅ Simple model works!\n",
      "  Logits: torch.Size([2])\n",
      "  Projections: torch.Size([2, 512])\n",
      "  Features: torch.Size([2, 256])\n",
      "✅ LoRA gradients working: 0 parameters have gradients\n"
     ]
    }
   ],
   "source": [
    "print(\"🔄 Creating simplest model...\")\n",
    "simple_model = IntegratedModel(lora_model, processor, model_downstream).to(device)\n",
    "\n",
    "waveforms, labels = next(iter(train_loader))\n",
    "waveforms = waveforms[:2].to(device)\n",
    "labels = labels[:2].to(device)\n",
    "\n",
    "try:\n",
    "    logits, projections, features = simple_model(waveforms, return_features=True)\n",
    "    print(f\"✅ Simple model works!\")\n",
    "    print(f\"  Logits: {logits.shape}\")\n",
    "    print(f\"  Projections: {projections.shape}\")\n",
    "    print(f\"  Features: {features.shape}\")\n",
    "    \n",
    "    simple_model.zero_grad()\n",
    "    loss, _, _ = criterion(logits, projections, labels)\n",
    "    loss.backward()\n",
    "    \n",
    "    grad_count = 0\n",
    "    for name, param in simple_model.named_parameters():\n",
    "        if 'lora' in name.lower() and param.grad is not None:\n",
    "            grad_norm = param.grad.norm().item()\n",
    "            if grad_norm > 1e-10:\n",
    "                grad_count += 1\n",
    "    \n",
    "    print(f\"✅ LoRA gradients working: {grad_count} parameters have gradients\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Simple model failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60af1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Start CL...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Epoch [1/50] - Contrastive Embedding Learning\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 2377/2377 [06:45<00:00,  5.86it/s, Total=0.1774, Cls=0.2957, Cont=0.0000]\n",
      "Validating: 100%|██████████| 2399/2399 [03:50<00:00, 10.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1] Summary:\n",
      "  🎯 Learning Rate: 1.00e-07\n",
      "  📈 Training   - Loss: 0.6598 (Cls: 0.9120, Cont: 0.2813)\n",
      "                 Acc: 0.8967, F1: 0.0061, UAR: 0.5007\n",
      "  📊 Validation - Loss: 0.6892, Acc: 0.8946, F1: 0.0000, UAR: 0.5000\n",
      "🌟 New best UAR: 0.5000, saving model...\n",
      "\n",
      "================================================================================\n",
      "Epoch [2/50] - Contrastive Embedding Learning\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 2377/2377 [06:46<00:00,  5.85it/s, Total=0.1014, Cls=0.1690, Cont=0.0000] \n",
      "Validating: 100%|██████████| 2399/2399 [03:54<00:00, 10.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [2] Summary:\n",
      "  🎯 Learning Rate: 1.00e-07\n",
      "  📈 Training   - Loss: 0.7535 (Cls: 1.0673, Cont: 0.2829)\n",
      "                 Acc: 0.8979, F1: 0.0000, UAR: 0.5000\n",
      "  📊 Validation - Loss: 0.8023, Acc: 0.8946, F1: 0.0000, UAR: 0.5000\n",
      "⏳ No improvement for 1/10 epochs\n",
      "\n",
      "================================================================================\n",
      "Epoch [3/50] - Contrastive Embedding Learning\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 2377/2377 [06:49<00:00,  5.81it/s, Total=0.0532, Cls=0.0887, Cont=0.0000] \n",
      "Validating: 100%|██████████| 2399/2399 [03:55<00:00, 10.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [3] Summary:\n",
      "  🎯 Learning Rate: 1.00e-07\n",
      "  📈 Training   - Loss: 0.8979 (Cls: 1.3170, Cont: 0.2693)\n",
      "                 Acc: 0.8979, F1: 0.0000, UAR: 0.5000\n",
      "  📊 Validation - Loss: 0.9376, Acc: 0.8946, F1: 0.0000, UAR: 0.5000\n",
      "⏳ No improvement for 2/10 epochs\n",
      "\n",
      "================================================================================\n",
      "Epoch [4/50] - Contrastive Embedding Learning\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 2377/2377 [06:48<00:00,  5.81it/s, Total=8.5861, Cls=14.3101, Cont=0.0000]\n",
      "Validating: 100%|██████████| 2399/2399 [03:55<00:00, 10.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [4] Summary:\n",
      "  🎯 Learning Rate: 1.00e-07\n",
      "  📈 Training   - Loss: 1.0572 (Cls: 1.5879, Cont: 0.2610)\n",
      "                 Acc: 0.8979, F1: 0.0000, UAR: 0.5000\n",
      "  📊 Validation - Loss: 1.0803, Acc: 0.8946, F1: 0.0000, UAR: 0.5000\n",
      "⏳ No improvement for 3/10 epochs\n",
      "\n",
      "================================================================================\n",
      "Epoch [5/50] - Contrastive Embedding Learning\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 2377/2377 [06:47<00:00,  5.84it/s, Total=0.0023, Cls=0.0039, Cont=0.0000] \n",
      "Validating: 100%|██████████| 2399/2399 [03:51<00:00, 10.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [5] Summary:\n",
      "  🎯 Learning Rate: 1.00e-07\n",
      "  📈 Training   - Loss: 1.1844 (Cls: 1.8085, Cont: 0.2484)\n",
      "                 Acc: 0.8979, F1: 0.0000, UAR: 0.5000\n",
      "  📊 Validation - Loss: 1.1813, Acc: 0.8946, F1: 0.0000, UAR: 0.5000\n",
      "⏳ No improvement for 4/10 epochs\n",
      "\n",
      "================================================================================\n",
      "Epoch [6/50] - Contrastive Embedding Learning\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6:   6%|▌         | 136/2377 [00:22<06:16,  5.95it/s, Total=0.0176, Cls=0.0293, Cont=0.0000] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     57\u001b[0m     preds \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39msigmoid(logits) \u001b[38;5;241m>\u001b[39m threshold)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m---> 58\u001b[0m     all_preds\u001b[38;5;241m.\u001b[39mextend(\u001b[43mpreds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     59\u001b[0m     all_labels\u001b[38;5;241m.\u001b[39mextend(labels\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     61\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "\n",
    "best_val_uar = 0.0  \n",
    "patience = 10\n",
    "early_stop_counter = 0\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_uar_scores = [] \n",
    "val_uar_scores = []    \n",
    "\n",
    "print(\"🚀 Start CL...\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    total_cls_loss = 0.0\n",
    "    total_cont_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    print(f'\\n{\"=\"*80}')\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] - Contrastive Embedding Learning')\n",
    "    print(f'{\"=\"*80}\\n')\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    \n",
    "    for batch_idx, (waveforms, labels) in enumerate(progress_bar):\n",
    "        \n",
    "        waveforms = waveforms.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits, projections, features = model(waveforms, return_features=True)\n",
    "        \n",
    "        if logits.dim() == 0:  \n",
    "            logits = logits.unsqueeze(0)  \n",
    "        elif logits.dim() == 1 and logits.shape[0] != labels.shape[0]:\n",
    "            logits = logits.repeat(labels.shape[0])\n",
    "\n",
    "        if labels.dim() == 0:\n",
    "            labels = labels.unsqueeze(0)\n",
    "\n",
    "        loss, cls_loss, cont_loss = criterion(logits, projections, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \"\"\"\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'lora' in name:\n",
    "                print(name, param.requires_grad, param.grad.norm() if param.grad is not None else \"NO GRAD\")\n",
    "        \"\"\"\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            preds = (torch.sigmoid(logits) > threshold).long()\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_cls_loss += cls_loss.item()\n",
    "        total_cont_loss += cont_loss.item()\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            'Total': f'{loss.item():.4f}',\n",
    "            'Cls': f'{cls_loss.item():.4f}',\n",
    "            'Cont': f'{cont_loss.item():.4f}'\n",
    "        })\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_cls_loss = total_cls_loss / len(train_loader)\n",
    "    avg_cont_loss = total_cont_loss / len(train_loader)\n",
    "    \n",
    "    train_acc = accuracy_score(all_labels, all_preds)\n",
    "    train_f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "    train_uar = recall_score(all_labels, all_preds, average='macro', zero_division=0)  # 添加UAR\n",
    "    \n",
    "    model.eval()\n",
    "    val_preds, val_labels = [], []\n",
    "    total_val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for waveforms, labels in tqdm(devel_loader, desc=\"Validating\"):\n",
    "            waveforms = waveforms.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            logits, projections, features = model(waveforms, return_features=True)\n",
    "            if logits.dim() == 0:  \n",
    "                logits = logits.unsqueeze(0)  \n",
    "            elif logits.dim() == 1 and logits.shape[0] != labels.shape[0]:\n",
    "                logits = logits.repeat(labels.shape[0])\n",
    "\n",
    "            if labels.dim() == 0:\n",
    "                labels = labels.unsqueeze(0)\n",
    "                \n",
    "            val_loss, val_cls_loss, val_cont_loss = criterion(logits, projections, labels)\n",
    "            total_val_loss += val_loss.item()\n",
    "            \n",
    "            preds = (torch.sigmoid(logits) > threshold).long()\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(devel_loader)\n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "    val_f1 = f1_score(val_labels, val_preds, zero_division=0)\n",
    "    val_uar = recall_score(val_labels, val_preds, average='macro', zero_division=0)  \n",
    "    \n",
    "    train_losses.append(avg_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    train_uar_scores.append(train_uar) \n",
    "    val_uar_scores.append(val_uar)      \n",
    "    \n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    print(f\"\\nEpoch [{epoch+1}] Summary:\")\n",
    "    print(f\"  🎯 Learning Rate: {current_lr:.2e}\")\n",
    "    print(f\"  📈 Training   - Loss: {avg_loss:.4f} (Cls: {avg_cls_loss:.4f}, Cont: {avg_cont_loss:.4f})\")\n",
    "    print(f\"                 Acc: {train_acc:.4f}, F1: {train_f1:.4f}, UAR: {train_uar:.4f}\")  \n",
    "    print(f\"  📊 Validation - Loss: {avg_val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}, UAR: {val_uar:.4f}\") \n",
    "    \n",
    "    if len(set(val_labels)) > 1 and len(set(val_preds)) > 1:\n",
    "        class_recalls = recall_score(val_labels, val_preds, average=None, zero_division=0)\n",
    "        print(f\"  🎯 Class Recalls - Healthy: {class_recalls[0]:.4f}, Cold: {class_recalls[1]:.4f}\")\n",
    "    \n",
    "    if val_uar > best_val_uar:\n",
    "        best_val_uar = val_uar\n",
    "        early_stop_counter = 0\n",
    "        torch.save(model.state_dict(), \"best_contrastive_embedding_model_lora.pth\")\n",
    "        print(f\"🌟 New best UAR: {best_val_uar:.4f}, saving model...\")\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        print(f\"⏳ No improvement for {early_stop_counter}/{patience} epochs\")\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"❌ Early stopping after {patience} epochs without improvement\")\n",
    "            break\n",
    "\n",
    "print(f\"\\n🎉 Finish Training! Best UAR: {best_val_uar:.4f}\")\n",
    "\n",
    "training_history = {\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'train_uar_scores': train_uar_scores,\n",
    "    'val_uar_scores': val_uar_scores,\n",
    "    'best_val_uar': best_val_uar,\n",
    "    'total_epochs': epoch + 1,\n",
    "    'early_stopped': early_stop_counter >= patience\n",
    "}\n",
    "\n",
    "torch.save(training_history, 'contrastive_training_history.pth')\n",
    "print(f\"💾 Training history saved to 'contrastive_training_history.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9608f5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# training_history = torch.load('contrastive_training_history.pth')\n",
    "# train_losses = training_history['train_losses']\n",
    "# val_losses = training_history['val_losses']\n",
    "# train_uar_scores = training_history['train_uar_scores']\n",
    "# val_uar_scores = training_history['val_uar_scores']\n",
    "\n",
    "# 创建图表\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Contrastive Learning Training Progress', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Loss曲线\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "ax1.plot(epochs, train_losses, 'b-o', label='Training Loss', linewidth=2, markersize=4)\n",
    "ax1.plot(epochs, val_losses, 'r-s', label='Validation Loss', linewidth=2, markersize=4)\n",
    "ax1.set_title('Training & Validation Loss', fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. UAR曲线\n",
    "ax2.plot(epochs, train_uar_scores, 'b-o', label='Training UAR', linewidth=2, markersize=4)\n",
    "ax2.plot(epochs, val_uar_scores, 'r-s', label='Validation UAR', linewidth=2, markersize=4)\n",
    "ax2.set_title('Training & Validation UAR', fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('UAR Score')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Loss详细分析\n",
    "ax3.plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "ax3.plot(epochs, val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "ax3.fill_between(epochs, train_losses, alpha=0.3, color='blue')\n",
    "ax3.fill_between(epochs, val_losses, alpha=0.3, color='red')\n",
    "ax3.set_title('Loss Curves (Filled)', fontweight='bold')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Loss')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. 最佳性能标注\n",
    "best_epoch = np.argmax(val_uar_scores) + 1\n",
    "best_uar = max(val_uar_scores)\n",
    "\n",
    "ax4.plot(epochs, val_uar_scores, 'g-o', label='Validation UAR', linewidth=3, markersize=6)\n",
    "ax4.axhline(y=best_uar, color='red', linestyle='--', alpha=0.7, label=f'Best UAR: {best_uar:.4f}')\n",
    "ax4.axvline(x=best_epoch, color='red', linestyle='--', alpha=0.7, label=f'Best Epoch: {best_epoch}')\n",
    "ax4.scatter([best_epoch], [best_uar], color='red', s=100, zorder=5)\n",
    "ax4.annotate(f'Best: {best_uar:.4f}\\nEpoch: {best_epoch}', \n",
    "             xy=(best_epoch, best_uar), xytext=(best_epoch+1, best_uar-0.02),\n",
    "             arrowprops=dict(arrowstyle='->', color='red'),\n",
    "             fontsize=10, fontweight='bold')\n",
    "ax4.set_title('Validation UAR with Best Performance', fontweight='bold')\n",
    "ax4.set_xlabel('Epoch')\n",
    "ax4.set_ylabel('UAR Score')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('contrastive_training_progress.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 打印训练总结\n",
    "print(\"📊 Training Summary:\")\n",
    "print(f\"   🏆 Best Validation UAR: {best_uar:.4f} (Epoch {best_epoch})\")\n",
    "print(f\"   📈 Final Training Loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"   📊 Final Validation Loss: {val_losses[-1]:.4f}\")\n",
    "print(f\"   🎯 Total Epochs: {len(train_losses)}\")\n",
    "if 'early_stopped' in locals() and early_stopped:\n",
    "    print(f\"   ⏹️  Early stopped: Yes\")\n",
    "else:\n",
    "    print(f\"   ⏹️  Early stopped: No\")\n",
    "\n",
    "# 额外的详细loss图\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# 分类损失和对比损失分别绘制（如果有的话）\n",
    "if 'total_cls_loss' in locals() and 'total_cont_loss' in locals():\n",
    "    plt.subplot(2, 2, 1)\n",
    "    # 这里需要你保存每个epoch的分类损失和对比损失\n",
    "    # 假设你有这些数据\n",
    "    plt.title('Classification vs Contrastive Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss差异图\n",
    "plt.subplot(2, 2, 2)\n",
    "loss_diff = np.array(val_losses) - np.array(train_losses)\n",
    "plt.plot(epochs, loss_diff, 'purple', linewidth=2, marker='o')\n",
    "plt.title('Validation - Training Loss Difference')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss Difference')\n",
    "plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# UAR改进图\n",
    "plt.subplot(2, 2, 3)\n",
    "if len(val_uar_scores) > 1:\n",
    "    uar_improvement = np.diff(val_uar_scores)\n",
    "    plt.bar(range(2, len(val_uar_scores)+1), uar_improvement, \n",
    "            color=['green' if x > 0 else 'red' for x in uar_improvement],\n",
    "            alpha=0.7)\n",
    "    plt.title('UAR Improvement by Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('UAR Change')\n",
    "    plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 学习曲线平滑图\n",
    "plt.subplot(2, 2, 4)\n",
    "# 应用移动平均平滑\n",
    "def moving_average(data, window=3):\n",
    "    if len(data) < window:\n",
    "        return data\n",
    "    return np.convolve(data, np.ones(window)/window, mode='valid')\n",
    "\n",
    "if len(train_losses) >= 3:\n",
    "    smooth_train = moving_average(train_losses, 3)\n",
    "    smooth_val = moving_average(val_losses, 3)\n",
    "    smooth_epochs = range(2, len(smooth_train)+2)\n",
    "    \n",
    "    plt.plot(smooth_epochs, smooth_train, 'b-', linewidth=3, label='Smoothed Training Loss')\n",
    "    plt.plot(smooth_epochs, smooth_val, 'r-', linewidth=3, label='Smoothed Validation Loss')\n",
    "    plt.title('Smoothed Loss Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('detailed_training_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"📈 训练图表已保存:\")\n",
    "print(\"   📊 contrastive_training_progress.png - 主要训练进度\")\n",
    "print(\"   📈 detailed_training_analysis.png - 详细分析图表\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
